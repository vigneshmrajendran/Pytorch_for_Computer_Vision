{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "import functools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Network(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Network, self).__init__()\n",
    "        self.conv1 = torch.nn.Conv2d(in_channels=1, out_channels=10, kernel_size=5)\n",
    "        self.conv2 = torch.nn.Conv2d(in_channels=10, out_channels=20, kernel_size=5)\n",
    "        self.conv3 = torch.nn.Conv2d(in_channels=20, out_channels=30, kernel_size=5)\n",
    "\n",
    "        self.fc1 = torch.nn.Linear(in_features=30*4*4, out_features=128)\n",
    "        self.fc2 = torch.nn.Linear(in_features=128, out_features=10)\n",
    "\n",
    "    def forward(t):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Network(\n",
      "  (conv1): Conv2d(1, 10, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2): Conv2d(10, 20, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv3): Conv2d(20, 30, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (fc1): Linear(in_features=480, out_features=128, bias=True)\n",
      "  (fc2): Linear(in_features=128, out_features=10, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "network = Network()\n",
    "print(network)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[[[ 0.0696,  0.1397, -0.1070,  0.0685,  0.0728],\n",
       "          [-0.0172,  0.0304, -0.1043, -0.0294,  0.0216],\n",
       "          [ 0.0922,  0.1093, -0.0095, -0.0862,  0.0972],\n",
       "          [-0.1005,  0.0687, -0.0308, -0.0536,  0.0168],\n",
       "          [ 0.1091, -0.1127, -0.1150, -0.0829, -0.0890]]],\n",
       "\n",
       "\n",
       "        [[[-0.0413, -0.1690,  0.0675,  0.0741, -0.0410],\n",
       "          [ 0.1363,  0.1618,  0.0063, -0.0943,  0.0557],\n",
       "          [-0.0358,  0.0774, -0.0388, -0.1385, -0.0352],\n",
       "          [ 0.0103,  0.0996,  0.0216,  0.0062, -0.0587],\n",
       "          [-0.1205,  0.0988,  0.0273, -0.1992, -0.0904]]],\n",
       "\n",
       "\n",
       "        [[[ 0.1164, -0.0738,  0.0761, -0.0042,  0.1734],\n",
       "          [-0.0770,  0.1194,  0.0738, -0.1738,  0.0536],\n",
       "          [ 0.1792, -0.1288, -0.0382, -0.0767, -0.0873],\n",
       "          [ 0.0069, -0.1926,  0.1858,  0.0532,  0.0231],\n",
       "          [ 0.1954,  0.0603, -0.1010,  0.1382,  0.0113]]],\n",
       "\n",
       "\n",
       "        [[[-0.1041, -0.1749,  0.0325, -0.1112, -0.0872],\n",
       "          [ 0.0129, -0.0899,  0.1517,  0.0722,  0.1090],\n",
       "          [-0.0953, -0.1040,  0.1074, -0.0095, -0.1266],\n",
       "          [-0.0809, -0.1228, -0.1568, -0.0928,  0.0702],\n",
       "          [ 0.0172, -0.0785, -0.1961,  0.1741, -0.0021]]],\n",
       "\n",
       "\n",
       "        [[[-0.1090, -0.0415,  0.1228, -0.1094,  0.0392],\n",
       "          [ 0.0224, -0.0953,  0.0367,  0.0628, -0.0713],\n",
       "          [ 0.1607,  0.0037, -0.1470, -0.0543,  0.0739],\n",
       "          [-0.0068,  0.0496, -0.0551, -0.1981,  0.0332],\n",
       "          [ 0.0131,  0.1361, -0.1098, -0.0855, -0.0583]]],\n",
       "\n",
       "\n",
       "        [[[-0.0131, -0.1445,  0.1094,  0.0613,  0.1603],\n",
       "          [-0.0083,  0.0198,  0.0704,  0.1680, -0.0160],\n",
       "          [ 0.0573,  0.1502,  0.0413, -0.0559, -0.1269],\n",
       "          [-0.1579, -0.1856, -0.0150, -0.1290,  0.0139],\n",
       "          [-0.0522,  0.1352, -0.1195, -0.1816,  0.0277]]],\n",
       "\n",
       "\n",
       "        [[[-0.0391,  0.1925, -0.0021,  0.0635,  0.0400],\n",
       "          [-0.0229,  0.0161,  0.1763,  0.1554,  0.1244],\n",
       "          [-0.1126, -0.1172,  0.1242, -0.0539,  0.1924],\n",
       "          [-0.0946,  0.0472, -0.0304,  0.0527,  0.1055],\n",
       "          [-0.1568,  0.1680, -0.0149, -0.0699,  0.0649]]],\n",
       "\n",
       "\n",
       "        [[[-0.1619, -0.0905,  0.1295,  0.0896,  0.0752],\n",
       "          [ 0.0564,  0.0356, -0.0702,  0.1935,  0.0666],\n",
       "          [-0.0730, -0.1493,  0.1192, -0.0585, -0.1600],\n",
       "          [-0.1830, -0.1794,  0.1845, -0.1069,  0.1174],\n",
       "          [ 0.1186,  0.0575, -0.0166,  0.0233,  0.0146]]],\n",
       "\n",
       "\n",
       "        [[[-0.1892,  0.1732,  0.1958, -0.1377,  0.1207],\n",
       "          [ 0.0957, -0.1076, -0.0711,  0.0783, -0.0938],\n",
       "          [-0.1520, -0.1414,  0.0104,  0.1249,  0.0995],\n",
       "          [-0.1188, -0.0375,  0.1225,  0.1844, -0.0379],\n",
       "          [ 0.0006, -0.0170,  0.1358, -0.0837,  0.1068]]],\n",
       "\n",
       "\n",
       "        [[[ 0.0145, -0.0244, -0.0414, -0.0099, -0.1573],\n",
       "          [-0.0102, -0.1093,  0.1022,  0.0478,  0.1312],\n",
       "          [ 0.1280, -0.0135, -0.0161, -0.0529, -0.1802],\n",
       "          [-0.0079, -0.0493,  0.1372, -0.0713,  0.0162],\n",
       "          [ 0.0487,  0.1015,  0.0425,  0.0718, -0.0119]]]], requires_grad=True)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "network.conv1.weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['__call__',\n",
       " '__class__',\n",
       " '__delattr__',\n",
       " '__dict__',\n",
       " '__dir__',\n",
       " '__doc__',\n",
       " '__eq__',\n",
       " '__format__',\n",
       " '__ge__',\n",
       " '__getattr__',\n",
       " '__getattribute__',\n",
       " '__gt__',\n",
       " '__hash__',\n",
       " '__init__',\n",
       " '__init_subclass__',\n",
       " '__le__',\n",
       " '__lt__',\n",
       " '__module__',\n",
       " '__ne__',\n",
       " '__new__',\n",
       " '__reduce__',\n",
       " '__reduce_ex__',\n",
       " '__repr__',\n",
       " '__setattr__',\n",
       " '__setstate__',\n",
       " '__sizeof__',\n",
       " '__str__',\n",
       " '__subclasshook__',\n",
       " '__weakref__',\n",
       " '_apply',\n",
       " '_backward_hooks',\n",
       " '_buffers',\n",
       " '_forward_hooks',\n",
       " '_forward_pre_hooks',\n",
       " '_get_name',\n",
       " '_load_from_state_dict',\n",
       " '_load_state_dict_pre_hooks',\n",
       " '_modules',\n",
       " '_named_members',\n",
       " '_parameters',\n",
       " '_register_load_state_dict_pre_hook',\n",
       " '_register_state_dict_hook',\n",
       " '_save_to_state_dict',\n",
       " '_slow_forward',\n",
       " '_state_dict_hooks',\n",
       " '_tracing_name',\n",
       " '_version',\n",
       " 'add_module',\n",
       " 'apply',\n",
       " 'buffers',\n",
       " 'children',\n",
       " 'conv1',\n",
       " 'conv2',\n",
       " 'conv3',\n",
       " 'cpu',\n",
       " 'cuda',\n",
       " 'double',\n",
       " 'dump_patches',\n",
       " 'eval',\n",
       " 'extra_repr',\n",
       " 'fc1',\n",
       " 'fc2',\n",
       " 'float',\n",
       " 'forward',\n",
       " 'half',\n",
       " 'load_state_dict',\n",
       " 'modules',\n",
       " 'named_buffers',\n",
       " 'named_children',\n",
       " 'named_modules',\n",
       " 'named_parameters',\n",
       " 'parameters',\n",
       " 'register_backward_hook',\n",
       " 'register_buffer',\n",
       " 'register_forward_hook',\n",
       " 'register_forward_pre_hook',\n",
       " 'register_parameter',\n",
       " 'requires_grad_',\n",
       " 'share_memory',\n",
       " 'state_dict',\n",
       " 'to',\n",
       " 'train',\n",
       " 'training',\n",
       " 'type',\n",
       " 'zero_grad']"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dir(network)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'conv1.weight': torch.Size([10, 1, 5, 5]),\n",
       " 'conv1.bias': torch.Size([10]),\n",
       " 'conv2.weight': torch.Size([20, 10, 5, 5]),\n",
       " 'conv2.bias': torch.Size([20]),\n",
       " 'conv3.weight': torch.Size([30, 20, 5, 5]),\n",
       " 'conv3.bias': torch.Size([30]),\n",
       " 'fc1.weight': torch.Size([128, 480]),\n",
       " 'fc1.bias': torch.Size([128]),\n",
       " 'fc2.weight': torch.Size([10, 128]),\n",
       " 'fc2.bias': torch.Size([10])}"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "{n: t.shape for n, t in list(network.named_parameters())}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Python objects are callable objects because of __call__ method\n",
    "\n",
    "# Tensors are not directly used as weights in the layers, \n",
    "# They are instatiated as \"Parameters\" and then used as weights.\n",
    "# this is because there is a \"register_parameter\" function in the nn.Module class,\n",
    "# that requires all the weights to be \"registered\". More details to follow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "weight_tensor = torch.tensor(\n",
    "[\n",
    "  [1,2,3,4],\n",
    "  [5,6,7,8],\n",
    "  [9, 10, 11, 12]\n",
    "], dtype = torch.float32\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_tensor = torch.tensor([1,2,3,4], dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 30.,  70., 110.])"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weight_tensor.matmul(data_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer = torch.nn.Linear(in_features=4, out_features=3, bias=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# each Linear layer has a shape of (out_features, in_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer.weight = torch.nn.Parameter(weight_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 30.,  70., 110.], grad_fn=<SqueezeBackward3>)"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "layer(data_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# each layer has a forward method, which never get explicitly called.\n",
    "# everytime you call the layer, __call__ method of the layer is called,\n",
    "# then that method calls the forard method for the layer object.\n",
    "# So all you need to do is just call the layer object.\n",
    "# Same applies for the network objects too"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# layers have weights, operations dont.\n",
    "# conv is a layer, maxpooling and activations are operations\n",
    "# operations are implemented using torch.nn.functional module (a.k.a F module)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rewriting Network class again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Network(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Network, self).__init__()\n",
    "        # Layers\n",
    "        self.conv1 = torch.nn.Conv2d(in_channels=1, out_channels=6, kernel_size=5)\n",
    "        self.conv2 = torch.nn.Conv2d(in_channels=6, out_channels=12, kernel_size=5)\n",
    "        self.fc1 = torch.nn.Linear(in_features=12*4*4, out_features=120)\n",
    "        self.fc2 = torch.nn.Linear(in_features=120, out_features=60)\n",
    "        self.fc3 = torch.nn.Linear(in_features=60, out_features=10)\n",
    "\n",
    "        #Operations\n",
    "        self.maxpool2d2k2s = functools.partial(F.max_pool2d, kernel_size=2, stride=2)\n",
    "        self.relu = F.relu\n",
    "        self.softmax = F.softmax\n",
    "\n",
    "    def forward(self, t):\n",
    "        t = self.conv1(t)\n",
    "        t = self.relu(t)\n",
    "        t = self.maxpool2d2k2s(t)\n",
    "\n",
    "        t = self.conv2(t)\n",
    "        t = self.relu(t)\n",
    "        t = self.maxpool2d2k2s(t)\n",
    "\n",
    "        t = t.reshape(-1, 12*4*4)\n",
    "        t = self.fc1(t)\n",
    "        t = self.relu(t)\n",
    "\n",
    "        t = self.fc2(t)\n",
    "        t = self.relu(t)\n",
    "\n",
    "        t = self.fc3(t)\n",
    "        return t\n",
    "#     t = self.softmax(t) #Not needed because the training loop has Crossentropy loss func\n",
    "\n",
    "network = Network()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# standard pytorch import statements\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "\n",
    "import functools\n",
    "# torch.set_grad_enabled(True)  #optional\n",
    "torch.set_printoptions(precision=4, linewidth=120)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.3.1+cpu\n",
      "0.4.2+cpu\n"
     ]
    }
   ],
   "source": [
    "print(torch.__version__)\n",
    "print(torchvision.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_set = torchvision.datasets.FashionMNIST(\n",
    "  root = './data/FashionMNIST',\n",
    "  train=True,\n",
    "  download=True,\n",
    "  transform = transforms.Compose([\n",
    "    transforms.ToTensor()\n",
    "  ])\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = next(iter(training_set))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "image, label = sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 28, 28])"
      ]
     },
     "execution_count": 169,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Network(\n",
       "  (conv1): Conv2d(1, 6, kernel_size=(5, 5), stride=(1, 1))\n",
       "  (conv2): Conv2d(6, 12, kernel_size=(5, 5), stride=(1, 1))\n",
       "  (fc1): Linear(in_features=192, out_features=120, bias=True)\n",
       "  (fc2): Linear(in_features=120, out_features=60, bias=True)\n",
       "  (fc3): Linear(in_features=60, out_features=10, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = torch.randn_like(image.unsqueeze(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 1, 28, 28])"
      ]
     },
     "execution_count": 171,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "start.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.07, -0.05,  0.17,  0.05, -0.07,  0.08, -0.13, -0.07, -0.08,  0.11]], grad_fn=<AddmmBackward>)"
      ]
     },
     "execution_count": 172,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "network(image.unsqueeze(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_loader = torch.utils.data.DataLoader(\n",
    "  training_set,\n",
    "  batch_size=10\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = next(iter(data_loader))\n",
    "images, labels = batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10, 1, 28, 28])"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "images.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10])"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([9, 0, 0, 3, 0, 2, 7, 2, 5, 5])"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10, 10])\n",
      "tensor([[ 0.00, -0.02,  0.13,  0.13, -0.11, -0.11, -0.07, -0.06, -0.07,  0.08],\n",
      "        [-0.02, -0.03,  0.12,  0.15, -0.09, -0.10, -0.08, -0.06, -0.07,  0.04],\n",
      "        [-0.03, -0.05,  0.11,  0.16, -0.10, -0.12, -0.10, -0.04, -0.06,  0.06],\n",
      "        [-0.02, -0.05,  0.11,  0.15, -0.11, -0.12, -0.09, -0.06, -0.07,  0.06],\n",
      "        [-0.03, -0.04,  0.13,  0.15, -0.08, -0.11, -0.09, -0.06, -0.07,  0.07],\n",
      "        [ 0.00, -0.04,  0.11,  0.15, -0.08, -0.10, -0.07, -0.06, -0.07,  0.06],\n",
      "        [-0.02, -0.04,  0.12,  0.14, -0.11, -0.12, -0.09, -0.08, -0.07,  0.07],\n",
      "        [-0.00, -0.04,  0.12,  0.14, -0.09, -0.09, -0.07, -0.06, -0.06,  0.07],\n",
      "        [-0.03, -0.03,  0.11,  0.16, -0.13, -0.11, -0.10, -0.04, -0.07,  0.05],\n",
      "        [-0.00, -0.02,  0.11,  0.13, -0.13, -0.09, -0.07, -0.05, -0.06,  0.06]], grad_fn=<AddmmBackward>)\n"
     ]
    }
   ],
   "source": [
    "pred = network(images)\n",
    "print(pred.shape)\n",
    "print(pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([2, 3, 3, 3, 3, 3, 3, 3, 3, 3])"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.argmax(pred, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1)"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.argmax(pred, dim=1).eq(labels).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.argmax(pred, dim=1).eq(labels).sum().item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_num_correct(pred, labels):\n",
    "    return torch.argmax(pred, dim=1).eq(labels).sum().item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_num_correct(pred, labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The size of the output dimension along any axes after a CNN layer/operation:\n",
    "\n",
    "O = ((n - f - 2p) / s) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch.autograd.grad_mode.set_grad_enabled at 0x7fce390cd588>"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.set_grad_enabled(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.281947612762451"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds = network(images)\n",
    "loss = F.cross_entropy(preds, labels)\n",
    "loss.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "print(network.conv1.weight.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10, 1, 5, 5])\n"
     ]
    }
   ],
   "source": [
    "print(network.conv1.weight.grad.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.Adam(network.parameters(), lr = 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.281947612762451"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_num_correct(pred, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.3102736473083496\n",
      "2.293565273284912\n"
     ]
    }
   ],
   "source": [
    "data_loader = torch.utils.data.DataLoader(training_set, batch_size=100)\n",
    "optimizer = optim.Adam(network.parameters(), lr=0.01)\n",
    "\n",
    "batch = next(iter(data_loader))\n",
    "images, labels = batch\n",
    "\n",
    "pred = network(images)\n",
    "loss = F.cross_entropy(pred, labels)\n",
    "print(loss.item())\n",
    "\n",
    "loss.backward()\n",
    "optimizer.step()\n",
    "\n",
    "pred = network(images)\n",
    "loss = F.cross_entropy(pred, labels)\n",
    "print(loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.3269104957580566\n",
      "1.6831839084625244\n",
      "1.1321316957473755\n",
      "1.059425711631775\n",
      "1.125962734222412\n",
      "1.127094030380249\n",
      "1.0762988328933716\n",
      "0.9790597558021545\n",
      "0.8695643544197083\n",
      "0.7778597474098206\n",
      "0.7108258605003357\n"
     ]
    }
   ],
   "source": [
    "data_loader = torch.utils.data.DataLoader(training_set, batch_size=100)\n",
    "optimizer = optim.Adam(network.parameters(), lr=0.01)\n",
    "\n",
    "for i in range(10):\n",
    "    batch = next(iter(data_loader))\n",
    "    images, labels = batch\n",
    "\n",
    "    pred = network(images)\n",
    "    loss = F.cross_entropy(pred, labels)\n",
    "    print(loss.item())\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "pred = network(images)\n",
    "loss = F.cross_entropy(pred, labels)\n",
    "print(loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:  0  loss:  1382.1912021636963  correct:  5954\n",
      "epoch:  1  loss:  1382.1912021636963  correct:  5954\n",
      "epoch:  2  loss:  1382.1912021636963  correct:  5954\n",
      "epoch:  3  loss:  1382.1912021636963  correct:  5954\n",
      "epoch:  4  loss:  1382.1912021636963  correct:  5954\n",
      "epoch:  5  loss:  1382.1912021636963  correct:  5954\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-19-8093785ec476>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0mtotal_correct\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0mloss_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mbatch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdata_loader\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m         \u001b[0mimages\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/Projects/fastai_all_materials/fastai_all_materials/lib/python3.6/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    344\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__next__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    345\u001b[0m         \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 346\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_fetcher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    347\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    348\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/Projects/fastai_all_materials/fastai_all_materials/lib/python3.6/site-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36mfetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     42\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mauto_collation\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/Projects/fastai_all_materials/fastai_all_materials/lib/python3.6/site-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     42\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mauto_collation\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/Projects/fastai_all_materials/fastai_all_materials/lib/python3.6/site-packages/torchvision/datasets/mnist.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, index)\u001b[0m\n\u001b[1;32m     86\u001b[0m             \u001b[0mtuple\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0mwhere\u001b[0m \u001b[0mtarget\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mindex\u001b[0m \u001b[0mof\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mtarget\u001b[0m \u001b[0;32mclass\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m         \"\"\"\n\u001b[0;32m---> 88\u001b[0;31m         \u001b[0mimg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtargets\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     89\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m         \u001b[0;31m# doing this so that it is consistent with all other datasets\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "new_network = Network()\n",
    "\n",
    "data_loader = torch.utils.data.DataLoader(training_set, batch_size=100)\n",
    "optimizer = optim.Adam(new_network.parameters(), lr=0.01)\n",
    "NUM_EPOCHS = 10\n",
    "\n",
    "for i in range(NUM_EPOCHS):\n",
    "    total_loss = 0\n",
    "    total_correct = 0\n",
    "    loss_list = []\n",
    "    for batch in data_loader:\n",
    "        \n",
    "        images, labels = batch\n",
    "\n",
    "        predictions = new_network(images)\n",
    "        loss = F.cross_entropy(predictions, labels)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "#         weight_update.append(old_weights == new_weights)\n",
    "\n",
    "        total_correct += get_num_correct(predictions, labels)\n",
    "        total_loss += loss.item()\n",
    "        loss_list.append(loss.item())\n",
    "\n",
    "    print(\"epoch: \", i,' loss: ', total_loss, ' correct: ', total_correct)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2.3059635162353516,\n",
       " 2.3099279403686523,\n",
       " 2.311288356781006,\n",
       " 2.308073043823242,\n",
       " 2.3052587509155273,\n",
       " 2.299856185913086,\n",
       " 2.30435848236084,\n",
       " 2.3113608360290527,\n",
       " 2.300299882888794,\n",
       " 2.3156239986419678,\n",
       " 2.3029723167419434,\n",
       " 2.310032606124878,\n",
       " 2.3170361518859863,\n",
       " 2.3073227405548096,\n",
       " 2.321134567260742,\n",
       " 2.305598258972168,\n",
       " 2.309094190597534,\n",
       " 2.3059206008911133,\n",
       " 2.309600830078125,\n",
       " 2.316791296005249,\n",
       " 2.2997803688049316,\n",
       " 2.313253402709961,\n",
       " 2.3110742568969727,\n",
       " 2.314945936203003,\n",
       " 2.3100972175598145,\n",
       " 2.3127431869506836,\n",
       " 2.2995803356170654,\n",
       " 2.304556131362915,\n",
       " 2.284764289855957,\n",
       " 2.2998709678649902,\n",
       " 2.311671018600464,\n",
       " 2.309366226196289,\n",
       " 2.305908203125,\n",
       " 2.3097825050354004,\n",
       " 2.2980103492736816,\n",
       " 2.3105008602142334,\n",
       " 2.289398193359375,\n",
       " 2.313318967819214,\n",
       " 2.3142566680908203,\n",
       " 2.321885585784912,\n",
       " 2.3086190223693848,\n",
       " 2.323125123977661,\n",
       " 2.3101603984832764,\n",
       " 2.3035483360290527,\n",
       " 2.3054797649383545,\n",
       " 2.303162097930908,\n",
       " 2.3187601566314697,\n",
       " 2.3051633834838867,\n",
       " 2.3067989349365234,\n",
       " 2.2997443675994873,\n",
       " 2.311966896057129,\n",
       " 2.306779384613037,\n",
       " 2.2983815670013428,\n",
       " 2.3077776432037354,\n",
       " 2.302067756652832,\n",
       " 2.3102545738220215,\n",
       " 2.3202621936798096,\n",
       " 2.299787759780884,\n",
       " 2.311551094055176,\n",
       " 2.315485954284668,\n",
       " 2.3020105361938477,\n",
       " 2.3156332969665527,\n",
       " 2.3168201446533203,\n",
       " 2.3169233798980713,\n",
       " 2.298689842224121,\n",
       " 2.3220412731170654,\n",
       " 2.299294948577881,\n",
       " 2.288602352142334,\n",
       " 2.3090410232543945,\n",
       " 2.315129041671753,\n",
       " 2.307927131652832,\n",
       " 2.303248405456543,\n",
       " 2.301084041595459,\n",
       " 2.30802059173584,\n",
       " 2.3124377727508545,\n",
       " 2.311915636062622,\n",
       " 2.2975106239318848,\n",
       " 2.3029444217681885,\n",
       " 2.3164286613464355,\n",
       " 2.3046278953552246,\n",
       " 2.2845888137817383,\n",
       " 2.30820369720459,\n",
       " 2.295576572418213,\n",
       " 2.2981553077697754,\n",
       " 2.3144466876983643,\n",
       " 2.3110122680664062,\n",
       " 2.290557384490967,\n",
       " 2.2991979122161865,\n",
       " 2.306600332260132,\n",
       " 2.29533314704895,\n",
       " 2.3036046028137207,\n",
       " 2.313119411468506,\n",
       " 2.291877031326294,\n",
       " 2.307662010192871,\n",
       " 2.3239431381225586,\n",
       " 2.32185697555542,\n",
       " 2.2946410179138184,\n",
       " 2.30238676071167,\n",
       " 2.3248589038848877,\n",
       " 2.3040966987609863,\n",
       " 2.3126766681671143,\n",
       " 2.293943166732788,\n",
       " 2.303412675857544,\n",
       " 2.3013198375701904,\n",
       " 2.3167781829833984,\n",
       " 2.3156933784484863,\n",
       " 2.2997207641601562,\n",
       " 2.2931694984436035,\n",
       " 2.2917330265045166,\n",
       " 2.306185483932495,\n",
       " 2.301880121231079,\n",
       " 2.300736904144287,\n",
       " 2.2895126342773438,\n",
       " 2.3150696754455566,\n",
       " 2.306450843811035,\n",
       " 2.3045995235443115,\n",
       " 2.301621198654175,\n",
       " 2.301121711730957,\n",
       " 2.2919459342956543,\n",
       " 2.2873764038085938,\n",
       " 2.31295108795166,\n",
       " 2.3044018745422363,\n",
       " 2.301487684249878,\n",
       " 2.3106703758239746,\n",
       " 2.3067739009857178,\n",
       " 2.307765245437622,\n",
       " 2.3171112537384033,\n",
       " 2.2957091331481934,\n",
       " 2.302584409713745,\n",
       " 2.3002758026123047,\n",
       " 2.3149826526641846,\n",
       " 2.2973756790161133,\n",
       " 2.3270833492279053,\n",
       " 2.3182380199432373,\n",
       " 2.3163225650787354,\n",
       " 2.297250509262085,\n",
       " 2.2845375537872314,\n",
       " 2.3080568313598633,\n",
       " 2.288658380508423,\n",
       " 2.3037188053131104,\n",
       " 2.3051300048828125,\n",
       " 2.3047237396240234,\n",
       " 2.3106775283813477,\n",
       " 2.3002126216888428,\n",
       " 2.3009204864501953,\n",
       " 2.3031136989593506,\n",
       " 2.3039731979370117,\n",
       " 2.31427264213562,\n",
       " 2.30033016204834,\n",
       " 2.3038134574890137,\n",
       " 2.300278663635254,\n",
       " 2.3095879554748535,\n",
       " 2.3113906383514404,\n",
       " 2.3052706718444824,\n",
       " 2.315582036972046,\n",
       " 2.3094263076782227,\n",
       " 2.3148162364959717,\n",
       " 2.297381639480591,\n",
       " 2.300494432449341,\n",
       " 2.3111319541931152,\n",
       " 2.32248854637146,\n",
       " 2.3024191856384277,\n",
       " 2.311250686645508,\n",
       " 2.315948486328125,\n",
       " 2.313119411468506,\n",
       " 2.3123044967651367,\n",
       " 2.309299945831299,\n",
       " 2.301138401031494,\n",
       " 2.297144889831543,\n",
       " 2.3085849285125732,\n",
       " 2.3031907081604004,\n",
       " 2.307239294052124,\n",
       " 2.325381278991699,\n",
       " 2.312434196472168,\n",
       " 2.30971360206604,\n",
       " 2.306579351425171,\n",
       " 2.2962169647216797,\n",
       " 2.3124101161956787,\n",
       " 2.310076951980591,\n",
       " 2.304690361022949,\n",
       " 2.3032543659210205,\n",
       " 2.298776149749756,\n",
       " 2.3082592487335205,\n",
       " 2.305781602859497,\n",
       " 2.3157520294189453,\n",
       " 2.3137094974517822,\n",
       " 2.3102152347564697,\n",
       " 2.310807228088379,\n",
       " 2.3110878467559814,\n",
       " 2.3032898902893066,\n",
       " 2.3011083602905273,\n",
       " 2.296252965927124,\n",
       " 2.3132212162017822,\n",
       " 2.30232572555542,\n",
       " 2.286834955215454,\n",
       " 2.300642728805542,\n",
       " 2.297868251800537,\n",
       " 2.307015895843506,\n",
       " 2.307112216949463,\n",
       " 2.2982022762298584,\n",
       " 2.317840337753296,\n",
       " 2.3244850635528564,\n",
       " 2.3032262325286865,\n",
       " 2.3047187328338623,\n",
       " 2.3101420402526855,\n",
       " 2.2921578884124756,\n",
       " 2.2969679832458496,\n",
       " 2.3009557723999023,\n",
       " 2.315995216369629,\n",
       " 2.311351776123047,\n",
       " 2.2900710105895996,\n",
       " 2.2961478233337402,\n",
       " 2.3141191005706787,\n",
       " 2.2948248386383057,\n",
       " 2.304572343826294,\n",
       " 2.318690061569214,\n",
       " 2.302485466003418,\n",
       " 2.3091962337493896,\n",
       " 2.3144562244415283,\n",
       " 2.323498487472534,\n",
       " 2.3094615936279297,\n",
       " 2.317410707473755,\n",
       " 2.2874557971954346,\n",
       " 2.314694881439209,\n",
       " 2.306692600250244,\n",
       " 2.312608480453491,\n",
       " 2.3249635696411133,\n",
       " 2.294053792953491,\n",
       " 2.3159618377685547,\n",
       " 2.2997865676879883,\n",
       " 2.2970876693725586,\n",
       " 2.324692487716675,\n",
       " 2.3165338039398193,\n",
       " 2.3126020431518555,\n",
       " 2.2980923652648926,\n",
       " 2.3047564029693604,\n",
       " 2.315338373184204,\n",
       " 2.303454637527466,\n",
       " 2.308223247528076,\n",
       " 2.3276031017303467,\n",
       " 2.2986528873443604,\n",
       " 2.3150687217712402,\n",
       " 2.3135671615600586,\n",
       " 2.3073782920837402,\n",
       " 2.2999799251556396,\n",
       " 2.293114185333252,\n",
       " 2.309265613555908,\n",
       " 2.3069915771484375,\n",
       " 2.3058323860168457,\n",
       " 2.306431770324707,\n",
       " 2.308140516281128,\n",
       " 2.3182201385498047,\n",
       " 2.314192771911621,\n",
       " 2.312131881713867,\n",
       " 2.297961473464966,\n",
       " 2.289402723312378,\n",
       " 2.3031232357025146,\n",
       " 2.3031997680664062,\n",
       " 2.3110055923461914,\n",
       " 2.304840326309204,\n",
       " 2.2893264293670654,\n",
       " 2.3003618717193604,\n",
       " 2.2886786460876465,\n",
       " 2.3018391132354736,\n",
       " 2.330291748046875,\n",
       " 2.298464059829712,\n",
       " 2.3137967586517334,\n",
       " 2.3100438117980957,\n",
       " 2.3114147186279297,\n",
       " 2.297358512878418,\n",
       " 2.3231842517852783,\n",
       " 2.3076393604278564,\n",
       " 2.293001890182495,\n",
       " 2.3163979053497314,\n",
       " 2.2981772422790527,\n",
       " 2.3065857887268066,\n",
       " 2.3054893016815186,\n",
       " 2.3183817863464355,\n",
       " 2.3258817195892334,\n",
       " 2.318495273590088,\n",
       " 2.3039913177490234,\n",
       " 2.3044121265411377,\n",
       " 2.2994484901428223,\n",
       " 2.300290107727051,\n",
       " 2.3122146129608154,\n",
       " 2.2951323986053467,\n",
       " 2.3051021099090576,\n",
       " 2.313356637954712,\n",
       " 2.3021132946014404,\n",
       " 2.3259387016296387,\n",
       " 2.315397262573242,\n",
       " 2.3103251457214355,\n",
       " 2.3101420402526855,\n",
       " 2.315462827682495,\n",
       " 2.3123300075531006,\n",
       " 2.3026654720306396,\n",
       " 2.3116984367370605,\n",
       " 2.3046770095825195,\n",
       " 2.303464889526367,\n",
       " 2.3164725303649902,\n",
       " 2.3247082233428955,\n",
       " 2.3111820220947266,\n",
       " 2.292787551879883,\n",
       " 2.2971086502075195,\n",
       " 2.319967746734619,\n",
       " 2.301534652709961,\n",
       " 2.3141298294067383,\n",
       " 2.3105576038360596,\n",
       " 2.2989675998687744,\n",
       " 2.312014102935791,\n",
       " 2.309136390686035,\n",
       " 2.313948392868042,\n",
       " 2.306164503097534,\n",
       " 2.3115479946136475,\n",
       " 2.3180370330810547,\n",
       " 2.3100175857543945,\n",
       " 2.3095335960388184,\n",
       " 2.2895102500915527,\n",
       " 2.311613082885742,\n",
       " 2.318450927734375,\n",
       " 2.2969248294830322,\n",
       " 2.3109307289123535,\n",
       " 2.307333469390869,\n",
       " 2.3005027770996094,\n",
       " 2.318971633911133,\n",
       " 2.3208353519439697,\n",
       " 2.294006109237671,\n",
       " 2.303987503051758,\n",
       " 2.3026647567749023,\n",
       " 2.2860817909240723,\n",
       " 2.3183932304382324,\n",
       " 2.298630475997925,\n",
       " 2.308121919631958,\n",
       " 2.307964324951172,\n",
       " 2.301222562789917,\n",
       " 2.2987122535705566,\n",
       " 2.304352045059204,\n",
       " 2.3176465034484863,\n",
       " 2.307420015335083,\n",
       " 2.3063905239105225,\n",
       " 2.2860686779022217,\n",
       " 2.3182082176208496,\n",
       " 2.3036797046661377,\n",
       " 2.2862842082977295,\n",
       " 2.316802978515625,\n",
       " 2.306929111480713,\n",
       " 2.298287868499756,\n",
       " 2.3113906383514404,\n",
       " 2.3064157962799072,\n",
       " 2.311652421951294,\n",
       " 2.308818817138672,\n",
       " 2.3010332584381104,\n",
       " 2.314450979232788,\n",
       " 2.3178601264953613,\n",
       " 2.2956595420837402,\n",
       " 2.2887697219848633,\n",
       " 2.2958929538726807,\n",
       " 2.30588436126709,\n",
       " 2.2898924350738525,\n",
       " 2.3093063831329346,\n",
       " 2.2959070205688477,\n",
       " 2.305361747741699,\n",
       " 2.311173915863037,\n",
       " 2.295968770980835,\n",
       " 2.289034366607666,\n",
       " 2.2886862754821777,\n",
       " 2.308642864227295,\n",
       " 2.3118767738342285,\n",
       " 2.299217939376831,\n",
       " 2.3134043216705322,\n",
       " 2.3062334060668945,\n",
       " 2.3067994117736816,\n",
       " 2.2923178672790527,\n",
       " 2.3102974891662598,\n",
       " 2.297013759613037,\n",
       " 2.321016788482666,\n",
       " 2.3083062171936035,\n",
       " 2.322376012802124,\n",
       " 2.3112621307373047,\n",
       " 2.3129851818084717,\n",
       " 2.3018481731414795,\n",
       " 2.3110220432281494,\n",
       " 2.303781270980835,\n",
       " 2.3107986450195312,\n",
       " 2.313239574432373,\n",
       " 2.292879104614258,\n",
       " 2.2903785705566406,\n",
       " 2.3291196823120117,\n",
       " 2.3071680068969727,\n",
       " 2.31180477142334,\n",
       " 2.3038737773895264,\n",
       " 2.3165931701660156,\n",
       " 2.2851815223693848,\n",
       " 2.3292148113250732,\n",
       " 2.3109426498413086,\n",
       " 2.31789493560791,\n",
       " 2.3059871196746826,\n",
       " 2.2988791465759277,\n",
       " 2.305172920227051,\n",
       " 2.303743839263916,\n",
       " 2.3229570388793945,\n",
       " 2.3072924613952637,\n",
       " 2.302661895751953,\n",
       " 2.2959070205688477,\n",
       " 2.313656806945801,\n",
       " 2.305100917816162,\n",
       " 2.29890513420105,\n",
       " 2.3176496028900146,\n",
       " 2.309086799621582,\n",
       " 2.312105417251587,\n",
       " 2.300133466720581,\n",
       " 2.291046380996704,\n",
       " 2.3046391010284424,\n",
       " 2.3084588050842285,\n",
       " 2.304007053375244,\n",
       " 2.311871290206909,\n",
       " 2.286843776702881,\n",
       " 2.314694404602051,\n",
       " 2.296118974685669,\n",
       " 2.3003249168395996,\n",
       " 2.3015363216400146,\n",
       " 2.3167388439178467,\n",
       " 2.3041555881500244,\n",
       " 2.300459146499634,\n",
       " 2.3038735389709473,\n",
       " 2.308091878890991,\n",
       " 2.3066725730895996,\n",
       " 2.290604591369629,\n",
       " 2.310490131378174,\n",
       " 2.300290584564209,\n",
       " 2.3070855140686035,\n",
       " 2.3148906230926514,\n",
       " 2.3014700412750244,\n",
       " 2.289593458175659,\n",
       " 2.30263090133667,\n",
       " 2.304107904434204,\n",
       " 2.3025259971618652,\n",
       " 2.303842306137085,\n",
       " 2.298706531524658,\n",
       " 2.3084349632263184,\n",
       " 2.307927131652832,\n",
       " 2.311602830886841,\n",
       " 2.300185203552246,\n",
       " 2.2904601097106934,\n",
       " 2.312154769897461,\n",
       " 2.3026230335235596,\n",
       " 2.2990453243255615,\n",
       " 2.315434217453003,\n",
       " 2.3108065128326416,\n",
       " 2.3108432292938232,\n",
       " 2.312736749649048,\n",
       " 2.3261303901672363,\n",
       " 2.3036611080169678,\n",
       " 2.294771671295166,\n",
       " 2.305563449859619,\n",
       " 2.322431802749634,\n",
       " 2.3025166988372803,\n",
       " 2.2973973751068115,\n",
       " 2.303074598312378,\n",
       " 2.3069512844085693,\n",
       " 2.303375005722046,\n",
       " 2.3190417289733887,\n",
       " 2.302198886871338,\n",
       " 2.3072350025177,\n",
       " 2.295842409133911,\n",
       " 2.3128957748413086,\n",
       " 2.3103060722351074,\n",
       " 2.335923194885254,\n",
       " 2.305866003036499,\n",
       " 2.314469814300537,\n",
       " 2.3262572288513184,\n",
       " 2.299393892288208,\n",
       " 2.3031630516052246,\n",
       " 2.3029682636260986,\n",
       " 2.304920196533203,\n",
       " 2.3154337406158447,\n",
       " 2.321986436843872,\n",
       " 2.2987899780273438,\n",
       " 2.310195207595825,\n",
       " 2.2987775802612305,\n",
       " 2.3153510093688965,\n",
       " 2.3098196983337402,\n",
       " 2.306574821472168,\n",
       " 2.3214728832244873,\n",
       " 2.3158626556396484,\n",
       " 2.3078081607818604,\n",
       " 2.3074123859405518,\n",
       " 2.3111021518707275,\n",
       " 2.3172967433929443,\n",
       " 2.309295654296875,\n",
       " 2.3136208057403564,\n",
       " 2.3198394775390625,\n",
       " 2.327977180480957,\n",
       " 2.292661190032959,\n",
       " 2.310962438583374,\n",
       " 2.33000111579895,\n",
       " 2.321237325668335,\n",
       " 2.3178365230560303,\n",
       " 2.3017454147338867,\n",
       " 2.304631471633911,\n",
       " 2.3031387329101562,\n",
       " 2.3044371604919434,\n",
       " 2.3159639835357666,\n",
       " 2.300091028213501,\n",
       " 2.3238942623138428,\n",
       " 2.300642251968384,\n",
       " 2.3177857398986816,\n",
       " 2.331202507019043,\n",
       " 2.3014204502105713,\n",
       " 2.3199472427368164,\n",
       " 2.3070926666259766,\n",
       " 2.3076882362365723,\n",
       " 2.3130428791046143,\n",
       " 2.320964813232422,\n",
       " 2.302765130996704,\n",
       " 2.2864999771118164,\n",
       " 2.3004953861236572,\n",
       " 2.3080742359161377,\n",
       " 2.3021633625030518,\n",
       " 2.299469232559204,\n",
       " 2.3041133880615234,\n",
       " 2.302163600921631,\n",
       " 2.3091907501220703,\n",
       " 2.3100290298461914,\n",
       " 2.3200151920318604,\n",
       " 2.3081319332122803,\n",
       " 2.3076255321502686,\n",
       " 2.297008752822876,\n",
       " 2.2833166122436523,\n",
       " 2.308436393737793,\n",
       " 2.3041975498199463,\n",
       " 2.3072757720947266,\n",
       " 2.319236993789673,\n",
       " 2.2987499237060547,\n",
       " 2.3004872798919678,\n",
       " 2.2927603721618652,\n",
       " 2.2880616188049316,\n",
       " 2.29866361618042,\n",
       " 2.3182015419006348,\n",
       " 2.296314001083374,\n",
       " 2.3013010025024414,\n",
       " 2.3110835552215576,\n",
       " 2.3028981685638428,\n",
       " 2.3067309856414795,\n",
       " 2.3195688724517822,\n",
       " 2.306126117706299,\n",
       " 2.2877447605133057,\n",
       " 2.309821605682373,\n",
       " 2.302572011947632,\n",
       " 2.308241367340088,\n",
       " 2.3145525455474854,\n",
       " 2.3074231147766113,\n",
       " 2.311424732208252,\n",
       " 2.300018548965454,\n",
       " 2.30584454536438,\n",
       " 2.3047056198120117,\n",
       " 2.3005058765411377,\n",
       " 2.2969772815704346,\n",
       " 2.3222427368164062,\n",
       " 2.278851270675659,\n",
       " 2.318530321121216,\n",
       " 2.29105806350708,\n",
       " 2.3066647052764893,\n",
       " 2.3137900829315186,\n",
       " 2.3204755783081055,\n",
       " 2.306424856185913,\n",
       " 2.309814691543579,\n",
       " 2.2995779514312744,\n",
       " 2.310917615890503,\n",
       " 2.2885019779205322,\n",
       " 2.3077754974365234,\n",
       " 2.3075692653656006,\n",
       " 2.3187618255615234,\n",
       " 2.3140411376953125,\n",
       " 2.323448419570923,\n",
       " 2.300863742828369,\n",
       " 2.3047263622283936,\n",
       " 2.300851583480835,\n",
       " 2.2935822010040283,\n",
       " 2.2964813709259033,\n",
       " 2.3052544593811035,\n",
       " 2.298793077468872,\n",
       " 2.296198844909668,\n",
       " 2.3059868812561035,\n",
       " 2.3049333095550537,\n",
       " 2.308048963546753,\n",
       " 2.3153066635131836,\n",
       " 2.2850234508514404,\n",
       " 2.2984700202941895,\n",
       " 2.312457799911499,\n",
       " 2.307689666748047,\n",
       " 2.3163273334503174,\n",
       " 2.297245740890503,\n",
       " 2.3083996772766113,\n",
       " 2.2999725341796875,\n",
       " 2.2966246604919434,\n",
       " 2.3180527687072754,\n",
       " 2.304150342941284,\n",
       " 2.3228631019592285,\n",
       " 2.3155086040496826]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[True, True, True, True, True],\n",
       "          [True, True, True, True, True],\n",
       "          [True, True, True, True, True],\n",
       "          [True, True, True, True, True],\n",
       "          [True, True, True, True, True]]],\n",
       "\n",
       "\n",
       "        [[[True, True, True, True, True],\n",
       "          [True, True, True, True, True],\n",
       "          [True, True, True, True, True],\n",
       "          [True, True, True, True, True],\n",
       "          [True, True, True, True, True]]],\n",
       "\n",
       "\n",
       "        [[[True, True, True, True, True],\n",
       "          [True, True, True, True, True],\n",
       "          [True, True, True, True, True],\n",
       "          [True, True, True, True, True],\n",
       "          [True, True, True, True, True]]],\n",
       "\n",
       "\n",
       "        [[[True, True, True, True, True],\n",
       "          [True, True, True, True, True],\n",
       "          [True, True, True, True, True],\n",
       "          [True, True, True, True, True],\n",
       "          [True, True, True, True, True]]],\n",
       "\n",
       "\n",
       "        [[[True, True, True, True, True],\n",
       "          [True, True, True, True, True],\n",
       "          [True, True, True, True, True],\n",
       "          [True, True, True, True, True],\n",
       "          [True, True, True, True, True]]],\n",
       "\n",
       "\n",
       "        [[[True, True, True, True, True],\n",
       "          [True, True, True, True, True],\n",
       "          [True, True, True, True, True],\n",
       "          [True, True, True, True, True],\n",
       "          [True, True, True, True, True]]]])"
      ]
     },
     "execution_count": 198,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weight_update"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.1"
      ]
     },
     "execution_count": 158,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total_correct/60000."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "del new_network"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
